{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "um_a-G-dFlm-",
        "EG39fpNPJ_n_",
        "OSZMQglMmhe0",
        "QALDBNY6BzIp",
        "Szr2dETKLBqD",
        "CrUecfL7VE-B",
        "5Aq59HAyBC8g",
        "ZSfZo-mx-4Gd",
        "nwVJF4EmY3UN",
        "awuTJxhXfHOV",
        "uQqxO5QirJ0Q",
        "smQRQUmIWWF8",
        "geru_dEwL_Hf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Instalando dependencias e iniciando bibliotecas"
      ],
      "metadata": {
        "id": "um_a-G-dFlm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1_bmEmRjGw2",
        "outputId": "8c36b9ed-d513-4995-edb6-fd1fbd6bebf4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install boto3\n",
        "!pip install awscli\n",
        "!pip install jsonlines"
      ],
      "metadata": {
        "id": "HXctLZp3J2xj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6c2fb08-7b44-4589-97e6-931fc502c2e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=5834e2f9f509beef010aba6b0ca1775e6bb2d57807e3436215fe830893bca935\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.34.59-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.35.0,>=1.34.59 (from boto3)\n",
            "  Downloading botocore-1.34.59-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
            "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.59->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.59->boto3) (2.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.59->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.34.59 botocore-1.34.59 jmespath-1.0.1 s3transfer-0.10.0\n",
            "Collecting awscli\n",
            "  Downloading awscli-1.32.59-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: botocore==1.34.59 in /usr/local/lib/python3.10/dist-packages (from awscli) (1.34.59)\n",
            "Collecting docutils<0.17,>=0.10 (from awscli)\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.2/548.2 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from awscli) (0.10.0)\n",
            "Requirement already satisfied: PyYAML<6.1,>=3.10 in /usr/local/lib/python3.10/dist-packages (from awscli) (6.0.1)\n",
            "Collecting colorama<0.4.5,>=0.2.5 (from awscli)\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting rsa<4.8,>=3.1.2 (from awscli)\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.34.59->awscli) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.34.59->awscli) (2.8.2)\n",
            "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore==1.34.59->awscli) (2.0.7)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.34.59->awscli) (1.16.0)\n",
            "Installing collected packages: rsa, docutils, colorama, awscli\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.18.1\n",
            "    Uninstalling docutils-0.18.1:\n",
            "      Successfully uninstalled docutils-0.18.1\n",
            "Successfully installed awscli-1.32.59 colorama-0.4.4 docutils-0.16 rsa-4.7.2\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0sALBlg6Fpep"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "660tudmgHsAH"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.context import SparkContext as sc\n",
        "from pyspark.sql.session import SparkSession\n",
        "import pyspark\n",
        "from pyspark.sql.functions import col, lit, collect_list\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, ArrayType, DoubleType, IntegerType, FloatType, BooleanType\n",
        "from pyspark.sql import functions as F\n",
        "import boto3\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "import glob\n",
        "import math\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSV"
      ],
      "metadata": {
        "id": "BoBxyG2nrGdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lendo a base de dados CSV"
      ],
      "metadata": {
        "id": "EG39fpNPJ_n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crir um codigo em pyspark para ler os dados de um csv numa pasta do google colab\n",
        "\n",
        "# Criar uma SparkSession\n",
        "spark = SparkSession.builder.appName(\"LeituraCSV\").getOrCreate()\n",
        "\n",
        "# Definir o caminho para o arquivo CSV\n",
        "caminho_csv = \"/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV/movies_romance_pronto.csv\"\n",
        "\n",
        "# Ler o arquivo CSV usando o Spark\n",
        "df = spark.read.csv(caminho_csv, header=True, inferSchema=True)\n",
        "\n",
        "# Imprimir o DataFrame\n",
        "df.show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncPmB3OMKG2U",
        "outputId": "d29c1f18-543f-44a7-8038-24c86f5cceb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+-------------+-----------------------+---------+-----------+\n",
            "|id       |tituloPincipal           |anoLancamento|genero                 |notaMedia|numeroVotos|\n",
            "+---------+-------------------------+-------------+-----------------------+---------+-----------+\n",
            "|tt0000009|Miss Jerry               |1894         |Romance                |5.3      |200        |\n",
            "|tt0001175|Camille                  |1912         |Drama,Romance          |5.3      |38         |\n",
            "|tt0001475|Amor fatal               |1911         |Drama,Romance          |7.3      |21         |\n",
            "|tt0003442|Tess of the D'Urbervilles|1913         |Drama,Romance          |5.9      |25         |\n",
            "|tt0004207|The Last Egyptian        |1914         |Adventure,Drama,Romance|5.2      |22         |\n",
            "+---------+-------------------------+-------------+-----------------------+---------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tirando Colunas Desnecessarias"
      ],
      "metadata": {
        "id": "OSZMQglMmhe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Inicialize a sessão do Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RemoveColumnsAndSaveCSV\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Caminho do arquivo CSV\n",
        "file_path = \"/content/drive/MyDrive/ProjetoBolsa/dados/dadosRaw/movies.csv\"\n",
        "\n",
        "# Ler o arquivo CSV\n",
        "df = spark.read.csv(file_path, sep='|', header=True)\n",
        "\n",
        "# Remover colunas\n",
        "columns_to_drop = ['tituloOriginal', 'tempoMinutos', 'generoArtista', 'personagem', 'nomeArtista', 'anoNascimento', 'anoFalecimento', 'profissao', 'titulosMaisConhecidos']\n",
        "df = df.drop(*columns_to_drop)\n",
        "\n",
        "# Converter DataFrame do Spark para DataFrame do Pandas\n",
        "df_pandas = df.toPandas()\n",
        "\n",
        "# Caminho para salvar o novo arquivo CSV\n",
        "output_path = \"/content/drive/MyDrive/ProjetoBolsa/dados/dadosRaw/movies_colunas.csv\"\n",
        "\n",
        "# Salvar o DataFrame resultante em um novo arquivo CSV usando Pandas\n",
        "df_pandas.to_csv(output_path, sep='|', index=False)\n",
        "\n",
        "# Encerrar a sessão do Spark\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "rqz8o41JmkMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.read_csv('/content/drive/MyDrive/ProjetoBolsa/dados/dadosRaw/movies.csv', sep='|')\\\n",
        "    .drop(columns=['tituloOriginal', 'tempoMinutos', 'generoArtista','personagem','nomeArtista','anoNascimento','anoFalecimento','profissao','titulosMaisConhecidos'])\\\n",
        "    .to_csv('/content/drive/MyDrive/ProjetoBolsa/dados/dadosRaw/movies_colunas.csv', sep='|', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dhGQX0-GQat",
        "outputId": "be7f09dd-25cd-4a6a-f6cf-4a376cc5359e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-105-c1b481e57f25>:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pd.read_csv('/content/drive/MyDrive/ProjetoBolsa/dados/dadosRaw/movies.csv', sep='|')\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtrando os Filmes com Genero Romance\n"
      ],
      "metadata": {
        "id": "QALDBNY6BzIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar a sessão Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Filtragem de Dados\") \\\n",
        "    .getOrCreate()\n",
        "# Caminho do arquivo CSV de origem\n",
        "caminho_arquivo_origem = '/content/drive/MyDrive/ProjetoBolsa/dados/dadosRaw/movies_colunas.csv'\n",
        "# Caminho completo do arquivo CSV de destino\n",
        "caminho_arquivo_destino = '/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV/movies_romance.csv'\n",
        "# Coluna que deseja filtrar\n",
        "coluna_alvo = 'genero'\n",
        "# Valor a ser filtrado\n",
        "valor_filtrado = 'Romance'\n",
        "\n",
        "try:\n",
        "    # Carregar o arquivo CSV como um DataFrame Spark\n",
        "    df = spark.read.option(\"delimiter\", \"|\").csv(caminho_arquivo_origem, header=True)\n",
        "    # Filtrar os dados com base na coluna alvo e no valor desejado\n",
        "    dados_filtrados = df.filter(col(coluna_alvo).contains(valor_filtrado))\n",
        "    # Converter o DataFrame Spark em um DataFrame pandas\n",
        "    dados_pandas = dados_filtrados.toPandas()\n",
        "    # Salvar os dados filtrados como um arquivo CSV\n",
        "    dados_pandas.to_csv(caminho_arquivo_destino, index=False)\n",
        "    print(\"Dados filtrados salvos com sucesso em\", caminho_arquivo_destino)\n",
        "except Exception as e:\n",
        "    print(\"Ocorreu um erro:\", e)\n",
        "# Encerrar a sessão Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "dsuWuzFkTUWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279e1c42-3769-419f-e14a-ed46ad77757d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados filtrados salvos com sucesso em /content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV/movies_romance.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exibindo dados"
      ],
      "metadata": {
        "id": "Szr2dETKLBqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"LeituraCSV\").getOrCreate()\n",
        "\n",
        "# Definir o caminho para o arquivo CSV\n",
        "caminho_csv = \"/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV/movies_romance.csv\"\n",
        "\n",
        "# Ler o arquivo CSV usando o Spark\n",
        "df_romance = spark.read.csv(caminho_csv, header=True, inferSchema=True)\n",
        "\n",
        "# Imprimir o DataFrame\n",
        "df_romance.show(5, truncate=False)\n"
      ],
      "metadata": {
        "id": "v6MmvqiAJmNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd836bf-a1d5-4906-a1bb-5d0630e61458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------+--------------------+-------------+------------+-------------+---------+-----------+-------------+--------------------------------------------------------------+-----------------+-------------+--------------+----------------------------+---------------------------------------+\n",
            "|id       |tituloPincipal|tituloOriginal      |anoLancamento|tempoMinutos|genero       |notaMedia|numeroVotos|generoArtista|personagem                                                    |nomeArtista      |anoNascimento|anoFalecimento|profissao                   |titulosMaisConhecidos                  |\n",
            "+---------+--------------+--------------------+-------------+------------+-------------+---------+-----------+-------------+--------------------------------------------------------------+-----------------+-------------+--------------+----------------------------+---------------------------------------+\n",
            "|tt0000009|Miss Jerry    |Miss Jerry          |1894         |45          |Romance      |5.3      |200        |actress      |Miss Geraldine Holbrook (Miss Jerry)                          |Blanche Bayliss  |1878         |1951          |actress                     |tt0000009                              |\n",
            "|tt0000009|Miss Jerry    |Miss Jerry          |1894         |45          |Romance      |5.3      |200        |actor        |Mr. Hamilton                                                  |William Courtenay|1875         |1933          |actor                       |tt0021535,tt0000009,tt0020355,tt0020403|\n",
            "|tt0000009|Miss Jerry    |Miss Jerry          |1894         |45          |Romance      |5.3      |200        |actor        |Chauncey Depew - the Director of the New York Central Railroad|Chauncey Depew   |1834         |1928          |actor,writer                |tt1076833,tt0490842,tt0000009,tt4484306|\n",
            "|tt0001175|Camille       |La dame aux camélias|1912         |\\N          |Drama,Romance|5.3      |38         |actor        |Armand Duval                                                  |Lou Tellegen     |1881         |1934          |actor,director,writer       |tt0017393,tt0011873,tt0001175,tt0232225|\n",
            "|tt0001175|Camille       |La dame aux camélias|1912         |\\N          |Drama,Romance|5.3      |38         |actress      |Marguerite Gauthier                                           |Sarah Bernhardt  |1844         |1923          |actress,writer,miscellaneous|tt0001175,tt0200269,tt0008359,tt0202209|\n",
            "+---------+--------------+--------------------+-------------+------------+-------------+---------+-----------+-------------+--------------------------------------------------------------+-----------------+-------------+--------------+----------------------------+---------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtrando os ID's unicos"
      ],
      "metadata": {
        "id": "CrUecfL7VE-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Id's unicos Pyspark CSV Romance"
      ],
      "metadata": {
        "id": "5Aq59HAyBC8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Este está correto porem gera varios arquivos\n",
        "def ler_ids_unicos_e_salvar(caminho_arquivo_csv, pasta_destino):\n",
        "    # Inicia uma sessão Spark\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Ler IDs Únicos e Salvar\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # Carrega o arquivo CSV como um DataFrame Spark\n",
        "    df = spark.read.option(\"delimiter\", \"|\").option(\"header\", \"true\").csv(caminho_arquivo_csv)\n",
        "\n",
        "\n",
        "    # Extrai os IDs únicos da coluna 'id' e realiza a operação de filtro em um único passo\n",
        "    df_resultado = df.dropDuplicates(['id'])\n",
        "\n",
        "    # Salva os dados dos IDs únicos como um arquivo CSV diretamente\n",
        "    df_resultado.write.option(\"header\", \"true\").csv(pasta_destino + '/movies_ids_unicos_py.csv')\n",
        "\n",
        "    # Encerra a sessão Spark\n",
        "    spark.stop()\n",
        "\n",
        "\n",
        "caminho_arquivo_csv = \"/content/drive/MyDrive/ProjetoBolsa/dados/dadosRaw/movies_colunas.csv\"\n",
        "pasta_destino = \"/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV\"\n",
        "ler_ids_unicos_e_salvar(caminho_arquivo_csv, pasta_destino)\n"
      ],
      "metadata": {
        "id": "af4vcuSQ_7GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### junção dos dados csv do pysarpk\n"
      ],
      "metadata": {
        "id": "3sqVkVRdFdfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def juntar_arquivos_csv(pasta_origem, nome_arquivo_saida):\n",
        "    # Lista todos os arquivos CSV na pasta de origem\n",
        "    arquivos_csv = glob.glob(pasta_origem + '/*.csv')\n",
        "\n",
        "    # Lista para armazenar os DataFrames de cada arquivo CSV\n",
        "    dfs = []\n",
        "\n",
        "    # Lê cada arquivo CSV e adiciona seu DataFrame à lista de DataFrames\n",
        "    for arquivo_csv in arquivos_csv:\n",
        "        df = pd.read_csv(arquivo_csv)\n",
        "        dfs.append(df)\n",
        "\n",
        "    # Concatena todos os DataFrames em um único DataFrame\n",
        "    df_concatenado = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Salva o DataFrame concatenado como um arquivo CSV\n",
        "    df_concatenado.to_csv(nome_arquivo_saida, index=False)\n",
        "\n",
        "# Pasta de origem contendo os arquivos CSV gerados pelo código anterior\n",
        "pasta_origem = \"/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV/movies_ids_unicos_py.csv\"\n",
        "\n",
        "# Nome do arquivo de saída que conterá todos os dados juntos\n",
        "nome_arquivo_saida = \"/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV/movies_ids_unicos.csv\"\n",
        "\n",
        "# Chama a função para juntar os arquivos CSV\n",
        "juntar_arquivos_csv(pasta_origem, nome_arquivo_saida)\n"
      ],
      "metadata": {
        "id": "HFm6gUFXBfJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"LeituraCSV\").getOrCreate()\n",
        "\n",
        "# Definir o caminho para o arquivo CSV\n",
        "caminho_csv = \"/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV/movies_ids_unicos.csv\"\n",
        "\n",
        "# Ler o arquivo CSV usando o Spark\n",
        "df_id = spark.read.csv(caminho_csv, header=True, inferSchema=True)\n",
        "\n",
        "# Imprimir o DataFrame\n",
        "df_id.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adsmaq0Brsaj",
        "outputId": "0f61ee2c-6c31-4032-a0f4-1a078c115c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------------------------+-------------+--------------------------+---------+-----------+\n",
            "|id       |tituloPincipal                |anoLancamento|genero                    |notaMedia|numeroVotos|\n",
            "+---------+------------------------------+-------------+--------------------------+---------+-----------+\n",
            "|tt0000009|Miss Jerry                    |1894         |Romance                   |5.3      |200        |\n",
            "|tt0000574|The Story of the Kelly Gang   |1906         |Action,Adventure,Biography|6.0      |797        |\n",
            "|tt0000591|The Prodigal Son              |1907         |Drama                     |5.1      |20         |\n",
            "|tt0000615|Robbery Under Arms            |1907         |Drama                     |4.3      |23         |\n",
            "|tt0000679|The Fairylogue and Radio-Plays|1908         |Adventure,Fantasy         |5.2      |66         |\n",
            "+---------+------------------------------+-------------+--------------------------+---------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limpando a tabela Id_Unicos"
      ],
      "metadata": {
        "id": "ZSfZo-mx-4Gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa uma SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Limpeza de Dados\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Caminho do arquivo CSV\n",
        "nome_arquivo = '/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV/movies_ids_unicos.csv'\n",
        "\n",
        "# Carrega os dados CSV para um DataFrame Spark\n",
        "dados = spark.read.csv(nome_arquivo, header=True, inferSchema=True)\n",
        "\n",
        "# Remove linhas com células vazias\n",
        "dados_limpos = dados.na.drop()\n",
        "\n",
        "# Coleta os dados limpos em um DataFrame Pandas\n",
        "dados_limpos_pandas = dados_limpos.toPandas()\n",
        "\n",
        "# Caminho para salvar o arquivo limpo\n",
        "pasta_destino = '/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV/'\n",
        "nome_arquivo_limpo = pasta_destino + 'movies_dados_id_limpo.csv'\n",
        "\n",
        "# Salva os dados limpos como arquivo CSV usando o Pandas\n",
        "dados_limpos_pandas.to_csv(nome_arquivo_limpo, index=False)\n",
        "\n",
        "print(\"Linhas com células vazias removidas e arquivo salvo com sucesso!\")\n",
        "\n",
        "# Encerra a SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtNjWuAUjZfQ",
        "outputId": "023b87f2-7052-438b-ed1e-e5a339f6b6cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linhas com células vazias removidas e arquivo salvo com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retirando os dados N"
      ],
      "metadata": {
        "id": "nwVJF4EmY3UN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicialize a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Remove linhas com valor 'N'\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Caminho para o arquivo CSV de entrada\n",
        "caminho_entrada = r\"/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV/movies_dados_id_limpo.csv\"\n",
        "\n",
        "# Caminho para o arquivo CSV de saída (sem linhas com '\\N')\n",
        "caminho_saida = r\"/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV/movies_pronto.csv\"\n",
        "\n",
        "# Lê o arquivo CSV como um DataFrame\n",
        "df = spark.read.csv(caminho_entrada, header=True)\n",
        "\n",
        "# Remove as linhas onde qualquer coluna contém '\\N'\n",
        "for coluna in df.columns:\n",
        "    df = df.filter(col(coluna) != '\\\\N')\n",
        "\n",
        "# Converter DataFrame do Spark para DataFrame do Pandas\n",
        "df_pandas = df.toPandas()\n",
        "\n",
        "# Salva o DataFrame resultante em outro arquivo CSV usando Pandas\n",
        "df_pandas.to_csv(caminho_saida, index=False)\n",
        "\n",
        "# Encerra a SparkSession\n",
        "spark.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "4mHANxCuc-xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"LeituraCSV\").getOrCreate()\n",
        "\n",
        "# Definir o caminho para o arquivo CSV\n",
        "caminho_csv = \"/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV/movies_romance_pronto.csv\"\n",
        "\n",
        "# Ler o arquivo CSV usando o Spark\n",
        "df_limpo = spark.read.csv(caminho_csv, header=True, inferSchema=True)\n",
        "\n",
        "# Imprimir o DataFrame\n",
        "df_limpo.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "tCvff2OoOxlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ebd44f0-ecfa-449a-ce77-e8c0d6f1289e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+-------------+-----------------------+---------+-----------+\n",
            "|id       |tituloPincipal           |anoLancamento|genero                 |notaMedia|numeroVotos|\n",
            "+---------+-------------------------+-------------+-----------------------+---------+-----------+\n",
            "|tt0000009|Miss Jerry               |1894         |Romance                |5.3      |200        |\n",
            "|tt0001175|Camille                  |1912         |Drama,Romance          |5.3      |38         |\n",
            "|tt0001475|Amor fatal               |1911         |Drama,Romance          |7.3      |21         |\n",
            "|tt0003442|Tess of the D'Urbervilles|1913         |Drama,Romance          |5.9      |25         |\n",
            "|tt0004207|The Last Egyptian        |1914         |Adventure,Drama,Romance|5.2      |22         |\n",
            "+---------+-------------------------+-------------+-----------------------+---------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Salvando em Parquet"
      ],
      "metadata": {
        "id": "TMZlcRT9TkNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Iniciar uma sessão Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Leitura de arquivo Parquet\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Ler o arquivo Parquet para um DataFrame Spark\n",
        "df_spark = spark.read.parquet(\"/content/drive/MyDrive/ProjetoBolsa/json/jsonTrusted/movies_romance_TMDB_json.parquet\")\n",
        "\n",
        "# Exibir o esquema do DataFrame\n",
        "df_spark.printSchema()\n",
        "\n",
        "# Exibir as primeiras linhas do DataFrame\n",
        "df_spark.show()\n",
        "\n",
        "# Parar a sessão Spark\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPJZQKGJfRPu",
        "outputId": "8a62c51b-4037-41ff-e75f-7e895e1af33a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- classificacao_media: double (nullable = true)\n",
            " |-- contagem_votos: long (nullable = true)\n",
            " |-- data_lancamento: string (nullable = true)\n",
            " |-- generos: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- id_imdb: string (nullable = true)\n",
            " |-- identificacao: long (nullable = true)\n",
            " |-- orcamento: long (nullable = true)\n",
            " |-- popularidade: double (nullable = true)\n",
            " |-- receita: long (nullable = true)\n",
            "\n",
            "+-------------------+--------------+---------------+--------------------+---------+-------------+---------+------------------+-------+\n",
            "|classificacao_media|contagem_votos|data_lancamento|             generos|  id_imdb|identificacao|orcamento|      popularidade|receita|\n",
            "+-------------------+--------------+---------------+--------------------+---------+-------------+---------+------------------+-------+\n",
            "|                5.0|             3|     1894-10-08|  [[10749, Romance]]|tt0000009|       356151|        0|             1.611|      0|\n",
            "|                1.0|             1|     1912-01-02|       [[18, Drama]]|tt0001175|       282871|        0|             1.469|      0|\n",
            "|                1.0|             1|     1913-09-01|[[10749, Romance]...|tt0003442|       290616|        0|             0.745|      0|\n",
            "|                4.0|             1|     1914-11-15|[[10749, Romance]...|tt0004545|       468031|    16988|             1.211|  87028|\n",
            "|                5.9|            11|     1914-11-09|       [[18, Drama]]|tt0004825|       130274|        0|             1.613|      0|\n",
            "|                6.0|            10|     1914-06-08|[[10749, Romance]...|tt0004838|       145747|        0|             2.465|      0|\n",
            "|                6.2|             6|     1915-04-22|[[10752, War], [1...|tt0005059|       415021|        0|             1.604|      0|\n",
            "|                5.6|             7|     1915-02-22|                  []|tt0005179|       200366|        0|             1.879|      0|\n",
            "|                0.0|             0|     1915-06-21|[[12, Adventure],...|tt0005196|       911920|        0|1.5750000000000002|      0|\n",
            "|                5.5|             2|     1915-12-06|[[10749, Romance]...|tt0005214|       270484|        0|              1.26|      0|\n",
            "|                5.5|             6|     1915-01-04|[[37, Western], [...|tt0005393|       402388|    15110|             1.205| 102224|\n",
            "|                4.0|             1|     1915-11-07|[[37, Western], [...|tt0005553|       938337|        0|             1.239|      0|\n",
            "|                3.5|             2|     1915-03-01|[[18, Drama], [10...|tt0005592|       820557|        0|             0.857|      0|\n",
            "|                0.0|             0|     1914-12-01|[[12, Adventure],...|tt0005604|       937228|        0|             0.877|      0|\n",
            "|                0.0|             0|     1915-09-02|[[12, Adventure],...|tt0005802|       954851|        0|             0.961|      0|\n",
            "|                0.0|             0|     1915-12-15|[[18, Drama], [10...|tt0006140|       563508|    22472|               1.4| 102437|\n",
            "|                0.0|             0|     1916-03-09|[[18, Drama], [37...|tt0006417|       457571|        0|0.6000000000000001|      0|\n",
            "|                0.0|             0|     1916-06-18|  [[10749, Romance]]|tt0006456|       291511|        0|             1.085|      0|\n",
            "|                5.7|             3|     1918-05-23|       [[18, Drama]]|tt0006481|       120665|        0|0.9520000000000001|      0|\n",
            "|                7.0|            13|     1916-01-30|[[18, Drama], [36...|tt0006614|       137946|        0|             0.869|      0|\n",
            "+-------------------+--------------+---------------+--------------------+---------+-------------+---------+------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#usando spark fica varios arquivos uma merda\n",
        "# Iniciar a sessão Spark\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Ler um arquivo CSV para criar um DataFrame Spark\n",
        "df = spark.read.csv(\"/content/drive/MyDrive/ProjetoBolsa/json/jsonPreTrusted/todos_filmes_romance_tmdb.json\", header=True, inferSchema=True)\n",
        "\n",
        "# Escrever o DataFrame como um arquivo Parquet\n",
        "df.write.parquet(\"/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrusted/movies_romance_limpo.parquet\")"
      ],
      "metadata": {
        "id": "YghTRsVQTnqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#usando spark fica varios arquivos uma merda\n",
        "# Iniciar a sessão Spark\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Ler um arquivo CSV para criar um DataFrame Spark\n",
        "df = spark.read.json(\"/content/drive/MyDrive/ProjetoBolsa/json/jsonPreTrusted/todos_filmes_romance_tmdb.json\")\n",
        "\n",
        "# Escrever o DataFrame como um arquivo Parquet\n",
        "df.write.parquet(\"/content/drive/MyDrive/ProjetoBolsa/json/jsonTrusted/filmes_romance_tmdb.json\")"
      ],
      "metadata": {
        "id": "iuQb42varDDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#usando pandas o mesmo fica numa taleba só\n",
        "def csv_to_parquet(csv,parquet):\n",
        "  df = pd.read_json(csv)\n",
        "  df.to_parquet(parquet, index=False)\n",
        "  print(\"Arquivo convertido com sucesso\")\n",
        "\n",
        "csv_arq=\"/content/drive/MyDrive/ProjetoBolsa/json/jsonPreTrusted/todos_filmes_romance_tmdb.json\"\n",
        "parquet_dest=\"/content/drive/MyDrive/ProjetoBolsa/json/jsonTrusted/filmes_romance_tmdb.json\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  csv_to_parquet(csv_arq,parquet_dest)\n",
        "\n"
      ],
      "metadata": {
        "id": "1CUGMhj0q0mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Excluindo coluna json-parquet e salvando como parquet"
      ],
      "metadata": {
        "id": "awuTJxhXfHOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar uma sessão Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Limpeza de colunas\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Ler o arquivo Parquet para um DataFrame Spark\n",
        "df_spark = spark.read.parquet(\"/content/drive/MyDrive/ProjetoBolsa/json/jsonTrusted/movies_romance_TMDB_json.parquet\")\n",
        "\n",
        "# Limpar colunas (substitua 'coluna_indesejada' pelo nome da coluna que você deseja remover)\n",
        "colunas_para_manter = [col for col in df_spark.columns if col != 'coluna_indesejada']\n",
        "df_limpo = df_spark.select(*colunas_para_manter)\n",
        "\n",
        "# Converter para um DataFrame Pandas\n",
        "df_pandas = df_limpo.toPandas()\n",
        "\n",
        "# Salvar o DataFrame Pandas como um arquivo Parquet\n",
        "df_pandas.to_parquet(\"/content/drive/MyDrive/ProjetoBolsa/json/jsonRefined/movies_romance_TMDB_json.parquet\")\n",
        "\n",
        "# Parar a sessão Spark\n",
        "spark.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "Nh3zEJrBfEof"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for trailing characters in the JSON file\n",
        "with open(csv_arq) as file:\n",
        "    data = file.read()\n",
        "\n",
        "if data.endswith('\\n'):\n",
        "    print(\"The file has a trailing newline character.\")\n",
        "    # Remove the trailing newline character\n",
        "    with open(csv_arq) as file:\n",
        "        data = file.read()\n",
        "        file.seek(0)\n",
        "        file.truncate()\n",
        "        file.write(data.rstrip())\n",
        "else:\n",
        "    print(\"The file does not have a trailing newline character.\")\n",
        "\n",
        "# Convert the JSON file to Parquet format\n",
        "def csv_to_parquet(json, parquet):\n",
        "    df = pd.read_json(json)\n",
        "    df.to_parquet(parquet, index=False)\n",
        "    print(\"Arquivo convertido com sucesso\")\n",
        "\n",
        "csv_to_parquet(csv_arq, parquet_dest)"
      ],
      "metadata": {
        "id": "2mLdBjbF1SNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Json - TMDB"
      ],
      "metadata": {
        "id": "uQqxO5QirJ0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crir um codigo em pyspark para ler os dados de um csv numa pasta do google colab\n",
        "\n",
        "# Criar uma SparkSession\n",
        "spark = SparkSession.builder.appName(\"LeituraCSV\").getOrCreate()\n",
        "\n",
        "# Definir o caminho para o arquivo CSV\n",
        "caminho_csv = \"/content/drive/MyDrive/ProjetoBolsa/json/jsonPreTrusted/filmes_romance_tmad_tr_limpo.json\"\n",
        "\n",
        "# Ler o arquivo CSV usando o Spark\n",
        "df = spark.read.json(caminho_csv)\n",
        "\n",
        "# Imprimir o DataFrame\n",
        "df.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6T8_vfJ0YKo",
        "outputId": "1e92d39c-0de8-435e-feb7-d611d9ec07e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------------+---------------+--------------------------------------------------+---------+-------------+---------+------------+-------+\n",
            "|classificacao_media|contagem_votos|data_lancamento|generos                                           |id_imdb  |identificacao|orcamento|popularidade|receita|\n",
            "+-------------------+--------------+---------------+--------------------------------------------------+---------+-------------+---------+------------+-------+\n",
            "|5.0                |3             |1894-10-08     |[[10749, Romance]]                                |tt0000009|356151       |0        |1.611       |0      |\n",
            "|1.0                |1             |1912-01-02     |[[18, Drama]]                                     |tt0001175|282871       |0        |1.469       |0      |\n",
            "|1.0                |1             |1913-09-01     |[[10749, Romance], [18, Drama]]                   |tt0003442|290616       |0        |0.745       |0      |\n",
            "|4.0                |1             |1914-11-15     |[[10749, Romance], [37, Western], [12, Adventure]]|tt0004545|468031       |16988    |1.211       |87028  |\n",
            "|5.9                |11            |1914-11-09     |[[18, Drama]]                                     |tt0004825|130274       |0        |1.613       |0      |\n",
            "+-------------------+--------------+---------------+--------------------------------------------------+---------+-------------+---------+------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#API TMDB\n",
        "api_key = \"6ac09baadc310ae1df51dfe6d4f6e4ea\"\n",
        "#URL base do TMDB\n",
        "base_url = \"https://api.themoviedb.org/3/\""
      ],
      "metadata": {
        "id": "FY9A9sYmrgl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criação JSON dos dados limpos"
      ],
      "metadata": {
        "id": "0__aHBrNwmPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def buscar_filme_por_id(api_key, movie_id):\n",
        "    url = f\"https://api.themoviedb.org/3/movie/{movie_id}?api_key={api_key}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def salvar_json(data, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "def processar_batch(batch_ids, api_key):\n",
        "    batch_films = []\n",
        "    for movie_id in batch_ids:\n",
        "        film = buscar_filme_por_id(api_key, movie_id)\n",
        "        if film:\n",
        "            batch_films.append(film)\n",
        "        else:\n",
        "            print(f\"Filme {movie_id} não encontrado.\")\n",
        "    return batch_films\n",
        "\n",
        "def main():\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Processamento de Filmes\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    api_key = \"6ac09baadc310ae1df51dfe6d4f6e4ea\"\n",
        "\n",
        "    # Caminho para o arquivo CSV\n",
        "    csv_filename = '/content/drive/MyDrive/ProjetoBolsa/dados/dadosRefined/dados_id_limpo.csv'\n",
        "\n",
        "    # Lendo os IDs da coluna \"id\" do arquivo CSV usando PySpark DataFrame\n",
        "    df = spark.read.option(\"header\", \"true\").csv(csv_filename)\n",
        "    movie_ids = [row['id'] for row in df.select(\"id\").collect()]\n",
        "    batch_size = 100\n",
        "    num_batches = math.ceil(len(movie_ids) / batch_size)\n",
        "    output_folder = Path('/content/drive/MyDrive/ProjetoBolsa/dados/dadosRaw/')  # Pasta de saída dos arquivos JSON\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(movie_ids))\n",
        "        batch_ids = movie_ids[start_idx:end_idx]\n",
        "\n",
        "        batch_rdd = spark.sparkContext.parallelize(batch_ids)\n",
        "        batch_films_rdd = batch_rdd.map(lambda movie_id: buscar_filme_por_id(api_key, movie_id))\n",
        "        batch_films = batch_films_rdd.collect()\n",
        "\n",
        "        if batch_films:\n",
        "            batch_filename = output_folder / f\"dados_filmes_romance_limpos_{i+1}.json\"\n",
        "            salvar_json(batch_films, batch_filename)\n",
        "            print(f\"Detalhes dos filmes {start_idx+1}-{end_idx} foram salvos em '{batch_filename}'.\")\n",
        "\n",
        "    spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "b0i8tAdnvMMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Json Linha Unica"
      ],
      "metadata": {
        "id": "yYoQcJLz4F7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def buscar_filme_por_id(api_key, movie_id):\n",
        "    url = f\"https://api.themoviedb.org/3/movie/{movie_id}?api_key={api_key}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def salvar_json(data, filename):\n",
        "    with open(filename, 'a') as f:\n",
        "        for item in data:\n",
        "            json.dump(item, f)\n",
        "            f.write('\\n')\n",
        "\n",
        "def processar_batch(batch_ids, api_key):\n",
        "    batch_films = []\n",
        "    for movie_id in batch_ids:\n",
        "        film = buscar_filme_por_id(api_key, movie_id)\n",
        "        if film:\n",
        "            batch_films.append(film)\n",
        "        else:\n",
        "            print(f\"Filme {movie_id} não encontrado.\")\n",
        "    return batch_films\n",
        "\n",
        "def main():\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Processamento de Filmes\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    api_key = \"6ac09baadc310ae1df51dfe6d4f6e4ea\"\n",
        "\n",
        "    # Caminho para o arquivo CSV\n",
        "    csv_filename = '/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV/movies_dados_id_limpo_romance.csv'\n",
        "\n",
        "    # Lendo os IDs da coluna \"id\" do arquivo CSV usando PySpark DataFrame\n",
        "    df = spark.read.option(\"header\", \"true\").csv(csv_filename)\n",
        "    movie_ids = [row['id'] for row in df.select(\"id\").collect()]\n",
        "    batch_size = 100\n",
        "    num_batches = math.ceil(len(movie_ids) / batch_size)\n",
        "    output_folder = Path('/content/drive/MyDrive/ProjetoBolsa/json/jsonRaw')  # Pasta de saída dos arquivos JSON\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(movie_ids))\n",
        "        batch_ids = movie_ids[start_idx:end_idx]\n",
        "\n",
        "        batch_rdd = spark.sparkContext.parallelize(batch_ids)\n",
        "        batch_films_rdd = batch_rdd.map(lambda movie_id: buscar_filme_por_id(api_key, movie_id))\n",
        "        batch_films = batch_films_rdd.collect()\n",
        "\n",
        "        if batch_films:\n",
        "            batch_filename = output_folder / f\"dados_filmes_romance_limpos_{i+1}.json\"\n",
        "            salvar_json(batch_films, batch_filename)\n",
        "            print(f\"Detalhes dos filmes {start_idx+1}-{end_idx} foram salvos em '{batch_filename}'.\")\n",
        "\n",
        "    spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "3MQMzGHL4FPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "def limpar_json(spark, input_folder, output_folder, keys_to_keep):\n",
        "    # Listar todos os arquivos JSON na pasta de entrada\n",
        "    input_files = glob.glob(input_folder + \"/*.json\")\n",
        "\n",
        "    for json_filename in input_files:\n",
        "        # Ler o arquivo JSON\n",
        "        df = spark.read.json(json_filename)\n",
        "\n",
        "        # Selecionar apenas as colunas desejadas\n",
        "        cleaned_df = df.select(*keys_to_keep)\n",
        "\n",
        "        # Converter o DataFrame do Spark para um DataFrame do Pandas\n",
        "        cleaned_df_pandas = cleaned_df.toPandas()\n",
        "\n",
        "        # Salvar o DataFrame limpo em um novo arquivo JSON usando o Pandas\n",
        "        output_filename = json_filename.split(\"/\")[-1].split(\".\")[0] + \".json\"\n",
        "        output_path = f\"{output_folder}/{output_filename}\"\n",
        "        cleaned_df_pandas.to_json(output_path, orient='records', lines=True)\n",
        "        print(f\"Arquivo limpo salvo em: {output_path}\")\n",
        "\n",
        "# Exemplo de utilização\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Limpeza de JSON\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "input_folder = '/content/drive/MyDrive/ProjetoBolsa/json/jsonTrusted'  # Pasta de entrada contendo os arquivos JSON\n",
        "output_folder = '/content/drive/MyDrive/ProjetoBolsa/json/jsonPreTrusted'  # Pasta de saída para os arquivos limpos\n",
        "keys_to_keep = [\"budget\", \"genres\", \"id\", \"imdb_id\", \"popularity\", \"release_date\", \"revenue\", \"vote_average\", \"vote_count\"]  # Chaves a serem mantidas\n",
        "\n",
        "limpar_json(spark, input_folder, output_folder, keys_to_keep)\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdTAA8Vx_Wtx",
        "outputId": "f182e3c2-64af-480c-92b4-7111df1104b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo limpo salvo em: /content/drive/MyDrive/ProjetoBolsa/json/jsonPreTrusted/todos_filmes_romance_tmdb.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dados do tmdb todos juntos"
      ],
      "metadata": {
        "id": "8UKBLN7TfCc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def buscar_filme_por_id(api_key, movie_id):\n",
        "    url = f\"https://api.themoviedb.org/3/movie/{movie_id}?api_key={api_key}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def salvar_json(data, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "def processar_batch(batch_ids, api_key):\n",
        "    batch_films = []\n",
        "    for movie_id in batch_ids:\n",
        "        film = buscar_filme_por_id(api_key, movie_id)\n",
        "        if film:\n",
        "            batch_films.append(film)\n",
        "        else:\n",
        "            print(f\"Filme {movie_id} não encontrado.\")\n",
        "    return batch_films\n",
        "\n",
        "def main():\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Processamento de Filmes\") \\\n",
        "        .getOrCreate()\n",
        "    # Caminho para o arquivo CSV\n",
        "    csv_filename = '/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV/dados_id_limpo.csv'\n",
        "\n",
        "    # Lendo os IDs da coluna \"id\" do arquivo CSV usando PySpark DataFrame\n",
        "    df = spark.read.option(\"header\", \"true\").csv(csv_filename)\n",
        "    movie_ids = [row['id'] for row in df.select(\"id\").collect()]\n",
        "    num_batches = math.ceil(len(movie_ids) / batch_size)\n",
        "    output_folder = Path('/content/drive/MyDrive/ProjetoBolsa/json/jsonRaw/aqui_dados/')  # Pasta de saída dos arquivos JSON\n",
        "\n",
        "    all_films = []\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(movie_ids))\n",
        "        batch_ids = movie_ids[start_idx:end_idx]\n",
        "\n",
        "        batch_rdd = spark.sparkContext.parallelize(batch_ids)\n",
        "        batch_films_rdd = batch_rdd.map(lambda movie_id: buscar_filme_por_id(api_key, movie_id))\n",
        "        batch_films = batch_films_rdd.collect()\n",
        "\n",
        "        if batch_films:\n",
        "            all_films.extend(batch_films)\n",
        "\n",
        "    output_filename = output_folder / \"todos_filmes_romance_limpos.csv\"\n",
        "    salvar_json(all_films, output_filename)\n",
        "    print(f\"Todos os detalhes dos filmes foram salvos em '{output_filename}'.\")\n",
        "\n",
        "    spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "us-XvHwWfCJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Juntando todos os arquivos da pasta"
      ],
      "metadata": {
        "id": "YjNqIuwkMFky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def buscar_filme_por_id(api_key, movie_id):\n",
        "    url = f\"https://api.themoviedb.org/3/movie/{movie_id}?api_key={api_key}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def salvar_json(data, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        for filme in data:\n",
        "            json.dump(filme, f)\n",
        "            f.write('\\n')\n",
        "\n",
        "def main():\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Processamento de Filmes\") \\\n",
        "        .getOrCreate()\n",
        "    # Caminho para o arquivo CSV\n",
        "    csv_filename = '/content/drive/MyDrive/ProjetoBolsa/dados/dadosTrustedCSV/movies_romance_pronto.csv'\n",
        "\n",
        "    # Lendo os IDs da coluna \"id\" do arquivo CSV usando PySpark DataFrame\n",
        "    df = spark.read.option(\"header\", \"true\").csv(csv_filename)\n",
        "    movie_ids = [row['id'] for row in df.select(\"id\").collect()]\n",
        "    output_folder = Path('/content/drive/MyDrive/ProjetoBolsa/json/jsonTrusted/')  # Pasta de saída do arquivo JSON\n",
        "\n",
        "    all_films = []\n",
        "\n",
        "    for movie_id in movie_ids:\n",
        "        film = buscar_filme_por_id(api_key, movie_id)\n",
        "        if film:\n",
        "            all_films.append(film)\n",
        "        else:\n",
        "            print(f\"Filme {movie_id} não encontrado.\")\n",
        "\n",
        "    output_filename = output_folder / \"todos_filmes_romance_tmdb.json\"\n",
        "    salvar_json(all_films, output_filename)\n",
        "    print(f\"Todos os detalhes dos filmes foram salvos em '{output_filename}'.\")\n",
        "\n",
        "    spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Skt1Ee5eRBwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convertendo os atributos"
      ],
      "metadata": {
        "id": "Uao3CehT2xLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def renomear_atributos(spark, arquivo_entrada, arquivo_saida):\n",
        "    # Lendo o arquivo JSON como DataFrame Spark\n",
        "    df = spark.read.json(arquivo_entrada)\n",
        "\n",
        "    # Mapeamento dos nomes antigos para os novos nomes dos atributos\n",
        "    mapeamento = {\n",
        "        \"budget\": \"orcamento\",\n",
        "        \"genres\": \"generos\",\n",
        "        \"id\": \"identificacao\",\n",
        "        \"imdb_id\": \"id_imdb\",\n",
        "        \"popularity\": \"popularidade\",\n",
        "        \"release_date\": \"data_lancamento\",\n",
        "        \"revenue\": \"receita\",\n",
        "        \"vote_average\": \"classificacao_media\",\n",
        "        \"vote_count\": \"contagem_votos\"\n",
        "    }\n",
        "\n",
        "    # Renomeando os atributos\n",
        "    for antigo_nome, novo_nome in mapeamento.items():\n",
        "        if antigo_nome in df.columns:\n",
        "            df = df.withColumnRenamed(antigo_nome, novo_nome)\n",
        "\n",
        "    # Convertendo DataFrame Spark para DataFrame pandas\n",
        "    df_pandas = df.toPandas()\n",
        "\n",
        "    # Salvando o DataFrame pandas como arquivo JSON\n",
        "    df_pandas.to_json(arquivo_saida, orient='records')\n",
        "\n",
        "# Criando uma sessão Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Renomear Atributos\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Exemplo de uso\n",
        "arquivo_entrada = \"/content/drive/MyDrive/ProjetoBolsa/json/jsonPreTrusted/todos_filmes_romance_tmdb.json\"\n",
        "arquivo_saida = \"/content/drive/MyDrive/ProjetoBolsa/json/jsonPreTrusted/filmes_romance_tmdb_traducao.json\"\n",
        "\n",
        "renomear_atributos(spark, arquivo_entrada, arquivo_saida)\n",
        "\n",
        "# Parando a sessão Spark\n",
        "spark.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "frcX_XZ221Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limpando NA"
      ],
      "metadata": {
        "id": "jIjANdlD9ryy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def limpar_dados_nulos(spark, arquivo_entrada, arquivo_saida):\n",
        "    # Lendo o arquivo JSON como DataFrame Spark\n",
        "    df = spark.read.json(arquivo_entrada)\n",
        "\n",
        "    # Convertendo DataFrame Spark para DataFrame pandas\n",
        "    df_pandas = df.toPandas()\n",
        "\n",
        "    # Removendo linhas com valores nulos em qualquer coluna\n",
        "    df_limpo = df_pandas.dropna()\n",
        "\n",
        "    # Salvando o DataFrame limpo como arquivo JSON\n",
        "    df_limpo.to_json(arquivo_saida, orient='records')\n",
        "\n",
        "# Criando uma sessão Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Limpar Dados Nulos\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Exemplo de uso\n",
        "arquivo_entrada = \"/content/drive/MyDrive/ProjetoBolsa/json/jsonPreTrusted/todos_filmes_romance_tmdb.json\"\n",
        "arquivo_saida = \"/content/drive/MyDrive/ProjetoBolsa/json/jsonPreTrusted/filmes_romance_tmad_limpo.json\"\n",
        "\n",
        "limpar_dados_nulos(spark, arquivo_entrada, arquivo_saida)\n",
        "\n",
        "# Parando a sessão Spark\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "OHHETOUM-AHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Salvando json como parquet"
      ],
      "metadata": {
        "id": "9tXTBJ2MB9QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def converter_json_para_parquet(arquivo_entrada, arquivo_saida):\n",
        "    # Lendo o arquivo JSON como DataFrame pandas\n",
        "    df = pd.read_json(arquivo_entrada)\n",
        "\n",
        "    # Salvando o DataFrame como arquivo Parquet\n",
        "    df.to_parquet(arquivo_saida)\n",
        "\n",
        "# Exemplo de uso\n",
        "arquivo_entrada_json = \"/content/drive/MyDrive/ProjetoBolsa/json/jsonPreTrusted/filmes_romance_tmad_tr_limpo.json\"\n",
        "arquivo_saida_parquet = \"/content/drive/MyDrive/ProjetoBolsa/json/jsonTrusted/movies_romance_TMDB_json.parquet\"\n",
        "\n",
        "converter_json_para_parquet(arquivo_entrada_json, arquivo_saida_parquet)\n"
      ],
      "metadata": {
        "id": "jVN24nojCAAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicialize uma sessão Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Consulta CSV com PySpark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Caminho do arquivo CSV\n",
        "caminho_arquivo = \"/content/drive/MyDrive/ProjetoBolsa/json/jsonPreTrusted/filmes_romance_tmad_tr_limpo.json\"\n",
        "\n",
        "# Carregue o arquivo CSV em um DataFrame Spark\n",
        "df = spark.read.json(caminho_arquivo)\n",
        "\n",
        "# Exiba o esquema do DataFrame\n",
        "df.printSchema()\n",
        "df.createOrReplaceTempView(\"dados_csv\")\n",
        "consulta_sql = \"SELECT id_imdb FROM dados_csv\"\n",
        "resultado = spark.sql(consulta_sql)\n",
        "resultado.show()\n",
        "\n",
        "# Encerre a sessão Spark\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "1a-Dn0kOWX8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51bc9cb3-02db-4c4a-c3ef-768d5f40c514"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- classificacao_media: double (nullable = true)\n",
            " |-- contagem_votos: long (nullable = true)\n",
            " |-- data_lancamento: string (nullable = true)\n",
            " |-- generos: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- id_imdb: string (nullable = true)\n",
            " |-- identificacao: long (nullable = true)\n",
            " |-- orcamento: long (nullable = true)\n",
            " |-- popularidade: double (nullable = true)\n",
            " |-- receita: long (nullable = true)\n",
            "\n",
            "+---------+\n",
            "|  id_imdb|\n",
            "+---------+\n",
            "|tt0000009|\n",
            "|tt0001175|\n",
            "|tt0003442|\n",
            "|tt0004545|\n",
            "|tt0004825|\n",
            "|tt0004838|\n",
            "|tt0005059|\n",
            "|tt0005179|\n",
            "|tt0005196|\n",
            "|tt0005214|\n",
            "|tt0005393|\n",
            "|tt0005553|\n",
            "|tt0005592|\n",
            "|tt0005604|\n",
            "|tt0005802|\n",
            "|tt0006140|\n",
            "|tt0006417|\n",
            "|tt0006456|\n",
            "|tt0006481|\n",
            "|tt0006614|\n",
            "+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicializando o S3"
      ],
      "metadata": {
        "id": "SEkdHgUbJn7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AWS_ACCESS_KEY_ID=\"\"  # Substitua '--' pelo ID da chave de acesso da AWS\n",
        "AWS_SECRET_ACCESS_KEY=\"\"  # Substitua '--' pela chave secreta de acesso da AWS\n",
        "AWS_SESSION_TOKEN=\"\"  # Substitua '--' pelo token de sessão da AWS, se necessário\n",
        "AWS_REGION='us-east-1'\n",
        "\n",
        "# iniciar sessão\n",
        "s3_client = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "    aws_session_token=AWS_SESSION_TOKEN,\n",
        "    region_name=AWS_REGION\n",
        ")"
      ],
      "metadata": {
        "id": "r2tj3mm8J9sv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enviando para o S3"
      ],
      "metadata": {
        "id": "-ahCMqUbLcb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Camada Raw"
      ],
      "metadata": {
        "id": "geru_dEwL_Hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurações\n",
        "BUCKET_NAME = ''\n",
        "LOCAL_CSV_FOLDER = ''\n",
        "\n",
        "for csv_file in ['']:  # Substitua pelos nomes dos arquivos CSV na sua pasta local\n",
        "\n",
        "    s3_key = f\"Raw/{csv_file.split('.')[0]}/{datetime.now().strftime('%Y')}/{datetime.now().strftime('%m')}/{datetime.now().strftime('%d')}/{csv_file}\"\n",
        "\n",
        "    s3_client.upload_file(f\"{LOCAL_CSV_FOLDER}/{csv_file}\", BUCKET_NAME, s3_key)\n",
        "\n",
        "    print(f\"Arquivo {csv_file} enviado para {s3_key} no S3.\")\n"
      ],
      "metadata": {
        "id": "lT7ax5wtLYvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_NAME = ''\n",
        "LOCAL_CSV_FOLDER = ''\n",
        "\n",
        "# Lista todos os arquivos CSV na pasta LOCAL_CSV_FOLDER\n",
        "csv_files = glob.glob(f\"{LOCAL_CSV_FOLDER}/*.json\")\n",
        "\n",
        "# Itera sobre os arquivos CSV\n",
        "for csv_file in csv_files:\n",
        "    # Extrai o nome do arquivo sem a extensão\n",
        "    csv_file_name = csv_file.split('/')[-1].split('.')[0]\n",
        "\n",
        "    # Constrói a chave S3\n",
        "    s3_key = f\"Raw/{csv_file.split('.')[0]}/{datetime.datetime.now().strftime('%Y-%m-%d')}/{csv_file}\"\n",
        "\n",
        "    # Faz o upload do arquivo para o Amazon S3\n",
        "    s3_client.upload_file(csv_file, BUCKET_NAME, s3_key)\n",
        "\n",
        "    # Imprime uma mensagem indicando o envio bem-sucedido\n",
        "    print(f\"Arquivo {csv_file} enviado para {s3_key} no S3.\")\n"
      ],
      "metadata": {
        "id": "FZHNKB-HD-Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Camada Trusted"
      ],
      "metadata": {
        "id": "1toDXfPUMD9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurações\n",
        "BUCKET_NAME = ''\n",
        "LOCAL_CSV_FOLDER = ''\n",
        "\n",
        "for csv_file in ['movies_limpo.parquet','movies_romance_limpo.parquet']:  # Substitua pelos nomes dos arquivos CSV na sua pasta local\n",
        "\n",
        "    s3_key = f\"Trusted/{csv_file.split('.')[0]}/{datetime.datetime.now().strftime('%Y-%m-%d')}/{csv_file}\"\n",
        "\n",
        "    s3_client.upload_file(LOCAL_CSV_FOLDER + '/' + csv_file, BUCKET_NAME, s3_key)\n",
        "\n",
        "    print(f\"Arquivo {csv_file} enviado para {s3_key} no S3.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YDs6hIILarx",
        "outputId": "7b27e26e-4c9d-4644-cd2f-23075c7344f1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo movies_limpo.parquet enviado para Trusted/movies_limpo/2024-03-09/movies_limpo.parquet no S3.\n",
            "Arquivo movies_romance_limpo.parquet enviado para Trusted/movies_romance_limpo/2024-03-09/movies_romance_limpo.parquet no S3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Camada  Refined"
      ],
      "metadata": {
        "id": "gydJeXm4MGsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurações\n",
        "BUCKET_NAME = ''\n",
        "LOCAL_CSV_FOLDER = ''\n",
        "\n",
        "for csv_file in ['movies_romance_limpo.parquet','movies_limpo.parquet']:  # Substitua pelos nomes dos arquivos CSV na sua pasta local\n",
        "\n",
        "    s3_key = f\"Refined/{csv_file.split('.')[0]}/{datetime.datetime.now().strftime('%Y-%m-%d')}/{csv_file}\"\n",
        "\n",
        "    s3_client.upload_file(LOCAL_CSV_FOLDER + '/' + csv_file, BUCKET_NAME, s3_key)\n",
        "\n",
        "    print(f\"Arquivo {csv_file} enviado para {s3_key} no S3.\")"
      ],
      "metadata": {
        "id": "6k_YBqBcMDE2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85204f82-cbbd-477f-843e-ccfe752bd745"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo movies_romance_limpo.parquet enviado para Refined/movies_romance_limpo/2024-03-09/movies_romance_limpo.parquet no S3.\n",
            "Arquivo movies_limpo.parquet enviado para Refined/movies_limpo/2024-03-09/movies_limpo.parquet no S3.\n"
          ]
        }
      ]
    }
  ]
}